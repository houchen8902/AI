## 什么是机器学习

#### 概念
机器学习是关于算法和概率模型的科学研究，计算机系统使用这些来有效地执行特定任务，不是使用明确的指令，而是依赖模式和推理。机器学习是人工智能的子集。机器学习算法依赖样本数据建立数学模型，用来在执行任务时做出预测和判断而不是依靠明确的指令。机器学习算法广泛地应用在各种应用中，如邮件过滤、计算机视觉等，在执行这类任务时编写明确的算法指令是不可行的。
机器学习和计算统计密切相关，计算统计侧重于使用计算机进行预测。数学优化研究为机器学习领域提供了方法、理论和应用领域。数据挖掘是机器学习中的一个研究领域，侧重于通过无监督学习进行探索性数据分析。
在跨业务问题的应用中，机器学习也称为预测分析。

#### 定义
机器学习这个名字是由Arthur Samuel于1959年创造的。
Tom M. Mitchell为机器学习领域研究的算法提供了一个被更广泛引用、更正式的定义：“A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.”对于某给定任务T，在合理的性能度量方案P的前提下，某计算机程序可以自主学习任务T的经验E，随着提供合适、优质、大量的经验E，该程序对于任务T的性能逐步提高。解读：随着任务的不断执行，经验的累积会带来计算机性能的提升。解读：我们使用计算机设计一个系统，使它能够根据提供的训练数据按照一定的方式来学习；随着训练次数的增加，该系统可以在性能上不断学习和改进；通过参数优化的学习模型，能够用于预测相关问题的输出。
其它定义包括：
“机器学习是对能通过经验自动改进的计算机算法的研究”
“机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。”
“机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能”

#### 历史
美国计算机游戏和人工智能领域的先驱亚瑟·塞缪尔于1959年在IBM工作时创造了“机器学习”一词。作为一项科学努力，机器学习源于对人工智能的追求。在人工智能作为学科的早期阶段，一些研究人员对机器从数据中学习感兴趣。他们试图用各种符号方法以及后来被称为“神经网络”的方法来解决问题，这些主要是感知器和其他模型，后来被发现是广义线性统计模型的重建，还采用了概率推理，特别是在自动医疗诊断中。
然而，逻辑的、基于知识的方法的越来越侧重导致AI和机器学习之间产生了裂痕。概率系统受到数据采集和表示的理论和实际问题的困扰。到1980年，专家系统已经成为人工智能的主导，统计方法被边缘化。符号/基于知识的学习在AI中继续进行，导致归纳逻辑编程，但更多的统计学研究应用在模式识别和信息检索，在AI核心领域之外。大约在同一时间，人工智能和计算机科学已经放弃了神经网络研究。这条线作为“连接主义”被来自其他学科的研究人员，包括Hopfield，Rumelhart和Hinton在AI/CS领域之外继续。他们的主要成功发生在20世纪80年代中期，重新定义了反向传播。
机器学习作为一个单独的领域重组，在20世纪90年代开始蓬勃发展。该领域将目标从实现人工智能转变为解决实际可解决的问题。它将焦点从它从AI继承的象征性方法转移到了从统计学和概率论中借鉴的方法和模型。它还受益于数字化信息的日益普及，以及通过互联网分发信息的能力。

#### 与其它领域的关系
**与数据挖掘的关系：**
机器学习和数据挖掘通常采用相同的方法并且显著重叠，但是机器学习侧重于预测，基于从训练数据中学习的已知属性，数据挖掘侧重于发现数据中的未知属性。数据挖掘使用了许多机器学习方法，但目标不同; 另一方面，机器学习也采用数据挖掘方法作为“无监督学习”或作为预处理步骤来提高学习的准确性。这两个研究领域之间的混淆很大程度上来自于他们使用的基本假设：在机器学习中，性能通常根据已知知识的再生能力进行评估，而在知识发现和数据挖掘（KDD）中，关键任务是发现以前未知的知识。根据已知知识进行评估，未知信息（无监督）方法将很容易被其他监督方法超越，而在典型的KDD任务中，由于训练数据不可用，不能使用监督方法。

**与优化的关系：**
机器学习与优化也有密切的关系：许多学习问题被公式化为训练集的一些损失函数的最小化。损失函数表示正在训练的模型的预测值与实际实例值之间的差异。这两个领域之间的差异源于概括的目标：虽然优化算法可以最小化训练集上的损失，但机器学习关注的是最小化看不见的样本的损失。

**与统计的关系：**
机器学习和统计是紧密相关的领域。迈克尔·乔丹认为，从方法论原则到理论工具，机器学习使用的思想在统计学中有很长的历史。
Leo Breiman区分了两种统计建模范例：数据模型和算法模型，其中“算法模型”或多或少意味着机器学习算法，如随机森林。
一些统计学家采用了机器学习的方法，产生了他们称之为统计学习的综合领域。

## 机器学习步骤

> 数据收集 (买菜)  
> 数据清洗 (洗菜)  
> 特征工程 (切菜)  
> 数据建模 (炒菜)  

#### 理论
学习者的目标是从经验中概括知识。这里的概括是指学习机器经过在学习数据组上学习之后可以在新的未知数据上做出准确判断。训练数据来源于不同分布区域，学习者需要据此总结出一个通用模型，在未来新数据上可以做出高效准确的预测。
机器学习算法及其性能的计算分析是理论计算机科学的一个分支，称为计算学习理论。由于训练集是有限的，未来是不确定的，因此学习理论通常不能保证算法的性能。相替代地，使用概率界定性能非常普遍。偏差方差分解是量化误差的一种方法。
为了在泛化中获得最佳性能，假设的复杂性应该与数据底层函数的复杂性相匹配。如果假设不如函数复杂，那么模型就不适合数据。如果响应时模型的复杂性增加，则训练误差减小。但如果假设过于复杂，那么模型就会过度拟合，而且泛化会更加糟糕。
除了性能界限，学习理论家还要研究时间复杂性和学习的可行性。在计算学习理论中，如果计算可以在多项式时间内完成，则认为计算是可行的。有两种时间复杂度结果。好的结果是可以在多项式时间内学习到目标函数。坏的结果是不能在多项式时间内学习到这些函数。

## 机器学习分类

### 根据任务分类
根据输入输出数据、待解决的问题的不同，机器学习分成几个大类：

**监督学习 (supervised learning)：**
在监督学习中，算法根据同时包含输入和所需输出的一组数据建立数学模型。每个训练样本包含一个或多个输入和一个对应输出，也称为监控信号。例如，如果任务是判断一张图片是否包含特定的物体，监督学习算法的训练数据需要一组包含或不包含特定物体的图片，并且每张图片都有一个标签表示它是否包含特定物体。
在数学模型中，每个训练样本由数组或向量表示，训练数据由矩阵表示。通过目标函数的迭代优化，监督学习算法学习到的函数可用于预测新输入样本的输出。最佳的函数是可以让算法准确地判断训练集以外新样本的输出值。
监督学习包括分类算法和回归算法。当输出限于一组有限的值时，使用分类算法。对于过滤电子邮件的分类算法，输入将是传入的电子邮件，输出将是用于存放电子邮件的文件夹的名称。 对于识别垃圾邮件的算法，输出将是“垃圾邮件”或“非垃圾邮件”的预测，由布尔值true和false表示。回归算法的输出是连续值，可以具有范围内的任何值。例如物体的温度，长度或价格。
相似性学习是与分类和回归密切相关的监督学习领域，但它的目标是使用相似性函数测量两个样本之间的相似性或相关性。它可用于排名，推荐系统，视觉识别跟踪，面部验证和说话人验证。

**半监督学习 (Semi-supervised learning)：**
半监督学习从不完整的训练数据中训练数学模型，有一部分训练数据没有标签。

**无监督学习 (unsupervised learning)：**
在无监督学习中，算法通过一组只包含输入，不包含对应输出的数据训练数学模型。
无监督学习用于查找数据中的结构，如数据点的分组和聚类。无监督学习可以发现数据中的模式，并将输入分组，像特征学习中一样。无监督学习算法不是响应反馈，而是识别数据中的共性，并基于每个新数据中是否存在这种共性来做出反应。聚类分析是将一组观测值分配到子集（称为聚类）中，以便同一聚类内的观测值根据一个或多个预先指定的标准相似，而从不同聚类中提取的观测值则不同。不同的聚类技术对数据的结构做出不同的假设，通常由一些相似性度量定义并且例如通过内部紧凑性或相同聚类的成员之间的相似性以及分离（聚类之间的差异）来评估。
其他方法基于估计的密度和图形连通性。
无监督学习的核心应用是在统计学中的密度估计领域，尽管无监督学习包括涉及总结和解释数据特征的其他领域。
降维是指减少一组数据中特征或输入数量的过程。

**强化学习 (Reinforcement learning)：**
强化学习算法在动态环境中以正或负强化的形式给出反馈，以最大化正向奖励。在机器学习中，环境通常表示为马尔可夫决策过程（MDP）。许多强化学习算法使用动态编程技术。
由于其一般性，该领域在许多其他学科中进行了研究，例如博弈论，控制理论，运筹学，信息论，基于模拟的优化，多智能体系统，群体智能，统计学和遗传算法。
用于自动驾驶车辆或人机对抗游戏。

**主动学习 (Active learning)：**
主动学习算法基于预算访问有限输入的期望输出（训练标签），并优化其将获得训练标签的输入的选择。当以交互方式使用时，这些可以呈现给人类用户以进行标记。

### 根据问题分类
机器学习算法常常用来解决的问题
分类、聚类、回归、异常检测、关联规则、强化学习、结构预测、特征学习、在线学习、半监督学习、语法归纳

**分类 (Classification)：**

**聚类 (Clustering)：**

**回归 (Regression)：**

**异常检测 (Anomaly detection)：**
在数据挖掘中，异常检测通过发现与大多数数据有显著不同的可怀疑项和发现异常。通常，异常项目表示诸如银行欺诈，结构缺陷，医疗问题或文本错误之类的问题。

**关联规则 (Association rules):**

**特征学习 (Feature learning):**
一些学习算法旨在发现在训练期间提供的输入数据的更好表示。经典示例包括主成分分析和聚类分析。特征学习算法（也称为表示学习算法）通常尝试在其输入中保留信息，但也以使其有用的方式对其进行变换，通常作为执行分类或预测之前的预处理步骤。这项技术可重建来自未知来源的输入数据。这取代了人工提取标签操作，并允许机器在学习特征的同时使用它们用于执行特定的任务。
特征学习可以是监督学习、也可以是无监督学习。在有监督特征学习中，特征是从带标签样本数据中习得的。包括人工神经网络，多层感知器和监督字典学习。在无监督的特征学习中，使用未标记的输入数据来学习特征。 示例包括字典学习，独立分量分析，自动编码器，矩阵分解和各种形式的聚类。
Manifold learning algorithms 在学习表示是低维时使用。稀疏编码算法在学习表示是稀疏的情况下使用，这意味着数学模型具有许多零。多线性子空间学习算法旨在直接从多维数据的张量表示中学习低维表示，而不将它们重新形成为更高维的向量。深度学习算法发现多级表示或特征层次结构，具有根据（或生成）较低级别特征定义的更高级别，更抽象的特征。 有人认为，智能机器是一种学习表示的机器，可以解开解释观测数据的变异的潜在因素。
诸如分类之为类的机器学习需要输入数据在数学和计算机上容易处理，这种要求促进了特征学习的发展。因为在真实世界的数据中，比如图片、视频、传感器数据并没有算法为它们定义特征。所以解决方法是通过检查发现这些数据中的特征而不用依靠特定的算法。

**稀疏字典学习：**
稀疏字典学习是一种特征学习方法，其中训练示例被表示为基函数的线性组合，并且被假设为稀疏矩阵。该方法具有很强的NP难度并且难以解决。用于稀疏字典学习的流行方法是K-SVD算法。稀疏字典学习已经在几种情况下应用。在分类中，问题是确定先前未见过的训练示例属于哪些类。稀疏字典学习也已应用于图像去噪。关键的想法是一个干净的图像补丁可以用图像字典稀疏地表示，但噪声不能。

## 机器学习算法分类

#### 重要机器学习算法
1. C4.5决策树
2. K-均值(K-means)
3. 支持向量机(SVM)
4. Apriori
5. 最大期望算法（EM）
6. PageRank算法
7. AdaBoost算法
8. k-近邻算法(kNN)
9. 朴素贝叶斯算法(NB)
10. 分类回归树(CART)

#### 监督学习
1. 分类算法
2. 回归算法
3. 集成算法
4. 决策树
5. 人工神经网络
6. 感知器 (Perceptron)
7. 相关向量机(RVM)
8. CART

#### 分类算法
1. k-近邻算法(kNN)
2. C4.5决策树
3. 朴素贝叶斯算法(NB)
4. Logistic回归算法
5. 支持向量机
6. AdaBoost集成方法

#### 回归算法
1. 线性回归
2. 逻辑回归（Logistic Regression）
3. 逐步式回归（Stepwise Regression）
4. 多元自适应回归样条（Multivariate Adaptive Regression Splines）
5. 本地散点平滑估计（Locally Estimated Scatterplot Smoothing）
6. 最小二乘法（Ordinary Least Square）

#### 集成算法
1. Bagging
2. Boosting
3. 随机森林（Random Forest）
4. AdaBoost
5. Bootstrapped Aggregation（Bagging）
6. 堆叠泛化（Stacked Generalization， Blending）
7. 梯度推进机（Gradient Boosting Machine, GBM）
8. Boosting with AdaBoost

#### 决策树学习
1. 分类及回归树（CART）
2. ID3 (Iterative Dichotomiser 3)
3. 决策树C4.5
4. CHAID
5. Desision Stump
6. 随机森林
7. 多元自适应回归样条
8. 梯度推进机
9. 袋装决策树（bagged decision trees）

#### 无监督学习
无监督学习问题可以有三种类型
关联：发现数据集合中的相关数据共现的概率。它广泛用于市场篮子分析。例如：如果顾客购买面包，他有80％的可能购买鸡蛋。
群集：对样本进行分组，使得同一个群集内的对象彼此之间的关系比另一个群集中的对象更为相似。
维度降低：维度降低意味着减少数据集的变量数量，同时确保重要的信息仍然传达。可以使用特征提取方法和特征选择方法来完成维度降低。特征选择选择原始变量的一个子集。特征提取执行从高维空间到低维空间的数据转换。例如：PCA算法是一种特征提取方法。
Apriori，K-means，PCA是无监督学习的例子
1. K-均值聚类算法(K-means)
2. 关联分析的Apriori算法
3. FP-Growth算法改进关联分析

#### 聚类分析 (Cluster analysis)
BIRCH、层次、k平均、期望最大化(EM)、DBSCAN、OPTICS、均值飘移

**BIRCH:**
利用层次方法的平衡迭代规约和聚类, BIRCH(balanced iterative reducing and clustering using hierarchies)。是一个非监督式分层聚类算法，于1996年由 Tian Zhang 提出。算法的优势在于能够利用有限的内存资源完成对大数据集的高质量的聚类。该算法通过构建聚类特征树（Clustering Feature Tree，简称CF Tree），在接下来的聚类过程中，直接对聚类特征进行聚类，而无需对原始数据集进行聚类。因此在多数情况下只需要扫描一次数据库即可进行聚类，IO成本与数据集尺寸呈线性关系。

**CURE**
CURE (Clustering Using REpresentatives) is an efficient data clustering algorithm for large databases. Compared with K-means clustering it is more robust to outliers and able to identify clusters having non-spherical shapes and size variances.

**Hierarchical clustering:**

**k-means:**

**最大期望算法 EM:**
Expectation–maximization algorithm

**DBSCAN**
DBSCAN，英文全写为Density-based spatial clustering of applications with noise ，是在 1996 年由Martin Ester, Hans-Peter Kriegel, Jörg Sander 及 Xiaowei Xu 提出的聚类分析算法， 这个算法是以密度为本的：给定某空间里的一个点集合，这算法能把附近的点分成一组（有很多相邻点的点），并标记出位于低密度区域的局外点（最接近它的点也十分远）。DBSCAN 是最常用的聚类分析算法之一，也是科学文章中最常引用的聚类分析算法之一。

**OPTICS:**
OPTICS（英语：Ordering points to identify the clustering structure）是由Mihael Ankerst，Markus M. Breunig，Hans-Peter Kriegel和Jörg Sander提出的基于密度的聚类分析算法。OPTICS并不依赖全局变量来确定聚类，而是将空间上最接近的点相邻排列，以得到数据集合中的对象的线性排序。排序后生成的序列存储了与相邻点之间的距离，并最终生成了一个 dendrogram 。OPTICS算法的思路与DBSCAN类似，但是解决了DBSCAN的一个主要弱点，即如何在密度变化的数据中取得有效的聚类。同时 OPTICS也避免了多数聚类算法中对输入参数敏感的问题。

**Mean shift:**

#### 降维 (Dimensionality reduction)
1. 因子分析
2. LASSO
3. 偏最小二乘回归（Partial Least Square Regression，PLS）
4. Sammon映射
5. 多维尺度（Multi-Dimensional Scaling, MDS）, 
6. 投影追踪（Projection Pursuit）
7. Factor analysis
8. 典型相关 CCA
9. 独立成分分析 ICA
10. 线性判别分析 LDA
11. NMF
12. 主成份分析（Principle Component Analysis， PCA)
13. t-SNE

#### Structured prediction
1. 概率图模型
2. 贝叶斯网络
3. 条件随机场 CRF
4. 隐马尔可夫模型 HMM

#### 异常检测
1. 最近邻居法 k-NN
2. 局部离群因子 (Local outlier factor)

#### 人工神经网络
1. 自编码器 Autoencoder
2. 深度学习
3. 多层感知器 Multilayer perceptron
4. 递归神经网络 RNN
5. 受限玻尔兹曼机 Restricted Boltzmann machine
6. 自组织映射（Self-Organizing Map, SOM）
7. 卷积神经网络 (Convolutional neural network, CNN)
8. 感知器神经网络（Perceptron Neural Network)
9. 反向传递（Back Propagation）
10. Hopfield网络
11. 学习矢量量化（Learning Vector Quantization， LVQ）
12. DeepDream

#### 强化学习
强化学习是一种机器学习算法，它允许代理根据当前状态决定最佳的下一个动作
强化算法通常通过反复试验来学习最佳行为。它们通常用于机器人的训练，机器人可以通过在碰到障碍物后接收负面反馈来学习避免碰撞。近期的alphago zero就是采用的强化学习的方法，来完成实验的
1. Q学习
2. SARSA
3. 时间差分学习 (Temporal difference learning)

#### 连续型数值的回归预测（监督学习）
1. 加权线性回归
2. 分类回归树(CART)

#### 基于实例的算法
1. K最近邻算法(knn)
K邻近算法使用整个数据集作为训练集，而不是将数据集分成训练集和测试集
当新的数据实例需要结果时，KNN算法遍历整个数据集，以找到新实例的k个最近的实例，或者与新记录最相似的k个实例，然后对于分类问题的结果（对于回归问题）或模式输出均值。
实例之间的相似度使用欧几里德距离和Hamming距离等度量来计算。
2. 学习矢量量化
3. 自组织映射算法

#### 正则化方法
1. Ridge Regression
2. LASSO
3. 弹性网络

#### 贝叶斯方法
1. 朴素贝叶斯算法
计算事件发生的概率
2. 平均单依赖估计（Averaged One-Dependence Estimators， AODE）
3. Bayesian Belief Network（BBN）

#### 基于核的算法
1. 支持向量机
2. 径向基函数
3. 线性判别分析 LDA

#### 关键规则学习
1. Aprioi算法
2. Eclat算法

#### 深度学习
1. 受限波尔兹曼机
2. Deep Belief Networks（DBN）
3. 卷积网络（Convolutional Network）
4. 堆栈式自动编码器（Stacked Auto-encoders

#### 其它算法
1. 遗传算法
2. PageRank
3. 奇异值分解（Singular Value Decomposition，SVD）
